\section{Logsistička regresija}
Algoritam od kog potiče logistička regresija se naziva linearna regresija,
koja predstavlja algoritam nadgledanog mašinskog učenja koji je našao primenu u
regresiji, njegova modifikacija nalazi primenu u rešavanju klasifikacionog
problema.  \\

Linearna regresija je predstavljena linearnom funkcijom odučivanja oblika:
\begin{equation}
  y \ = \ \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \cdot\cdot\cdot + \theta_n x_n + \epsilon
\end{equation}
gde $\epsilon$ predstavlja grešku koja ima normalnu raspodelu i predstavlja odstupanje
dobijene vrednosti u odnosu na izlaz $y$. \\

Jasno je da nam ovakva reprezentacija ne odgovara za klasifikovanje diskretnog
izlaza. Umesto direktnog predviđanja klase $Y$, logistička regresija modelira
verovatnoću $p\left(X\right)$ da $X$ pripada specifičnoj kategoriji.
Postavljanjem odgovarajuće granice ($threshold$) možemo izvršiti diskretizaciju
izlaza:
\begin{equation}
  Y =
    \begin{cases}
      0, & \text{if}\ p\left(x\right) < threshold \\
      1, & \text{otherwise}\
    \end{cases}
\end{equation}

Pitanje kojim želimo da se bavimo, je kako modelirati vezu između
$p\left(X\right) = \theta_0 + \theta_1  X$ i $X$? Zbog jednostavnosti problema
smatraćemo da izlaz ima binarnu vrednost, nula ili jedan. \\

Najjedenostavnije rešenje je korstiti linearnu regresiju za predstavljanje
verovatnoće:
\begin{equation} \label{eq:logisitc_regression}
  p\left(X\right) = \theta_0 + \theta_1 X
\end{equation}

Problem kod ovakvog predstavljanja je dobijanje vrednosti koja je negativna ili
veća od jedan za specifične ulaze, verovatnoća mora biti u intervalu $[0, 1]$.
Kako bismo izbegli ovaj problem, modeliraćemo verovatnoću $p\left(X\right)$ koristeći
funkciju koja daje izlaz između 0 i 1 za sve vrednosti $X$. Mnoge funkcije
zadovoljavaju ovu osobinu, u logsitičkoj regresiji koristimo logističku funkciju
(sigmoid):
\begin{equation}
  \sigma\left(x\right)=\frac{e^{t}}{1+e^{t}}
\end{equation}
zamenom u funkciji (\ref{eq:logisitc_regression}) dobijamo:
\begin{equation} \label{eq:p_x_logit}
  p\left(x\right) = \frac{e^{\theta_0 + \theta_1  X}}{1 + e^{\theta_0 + \theta_1  X}}
\end{equation}

Zbog matematičke pogodnosti verovatnoću možemo predstaviti preko šanse ($odds$):
\begin{equation}
  odds = \frac{p\left(x\right)}{1-p\left(x\right)}
\end{equation}
primenom ove formule nad (\ref{eq:p_x_logit}) dobijamo:
\begin{equation}
  \frac{p\left(x\right)}{1-p\left(x\right)} = e^{\theta_0 + \theta_1  X}
\end{equation}
zatim logaritmovanjem:
\begin{equation}
  \log \frac{p\left(x\right)}{1-p\left(x\right)} = \theta_0+ \theta_1 X
\end{equation}
dobijamo funkciju koja se naziva $logit$ i koja je linearna po $X$. \\

Za procenu parametara $\theta_0$ i $\theta_1$ koristimo likelihood funkciju:
\begin{equation} \label{eq:maximum_likelihood}
  l\left(\theta_0, \theta_1\right) =
  \prod_{i:y_i = 1} p\left(x_i\right)
  \prod_{\hat{i}:\hat{y_i} = 0} \left(1 - p\left(\hat{x_i}\right)\right)
\end{equation}
$\theta_0$ i $\theta_1$ se biraju tako da maksimizuju (\ref{eq:maximum_likelihood}). \\

Generalizacijom ovog modela dobijamo multiple logistic regression:
\begin{equation}
  \log \frac{p\left(x\right)}{1-p\left(x\right)} = \theta_0+ \theta_1 X_1 + \cdot\cdot\cdot + \theta_p X_p
\end{equation}
gde je $X = (X_1, X_2, \cdot\cdot\cdot, X_p)$, a $p$ redni proj prediktora.
Ova jednačina se može zapisati kao:
\begin{equation} \label{eq:logistic_function}
  p(x) = \frac{e^{\theta_0 + \theta_1 X_1 + \cdot\cdot\cdot + \theta_p X_p}}{1 + e^{\theta_0 + \theta_1  X_1 + \cdot\cdot\cdot + \theta_p X_p}}
\end{equation}
isto kao i u prethodnom primeru, procena $\theta_0, \theta_1, \cdot\cdot\cdot \theta_p$ se vrši
korišćenjem maximum likelihood funkcije. \\

Nedostatak logistička regresije se javlja kod klasifikovanja odziva koji može da
ima više od dve klase. Sa ovim modelom se mora pribegavati višestrukim
korišćenjem binarne klasifikacije (strategije one versus all, one versu one).
Metod koji je obrađen u nastavku, linearna diskriminantna analiza, je pogodniji
za multiple-class klasifikaciju. \\
