\section{Mašine potpornih vektora - SVM}
Mašine potpornih vektora (support vecotr machine - SVM) spadaju u grupu neprobalističkih modela nadgledanog obučavanja i
predstavlja binarni linearni klasifikator - klasifikuje objekte u dve kategorije. SVM je generalizacija jednostavnog klasifikatora koji se naziva
\textit{maximal margin classifier}.

\subsection{\textit{Maximal Margin Classifier}}
U nastavku ovog poglavlja opisan je proces klasifikacije uz pomoć hiperravni, kao i koncept optimalnog konstruisanja klasifikatora.
\subsubsection{Sta je hiperravn?}
Hiperravan je ravan koja je u p-dimenzionalnom prostoru $p-1$ dimenzionalni podprostor. Na primer,
u dvodimenzionalnom prostru hiperravan je jednodimenzionalni podporstor, dok je u trodimenzionalnom prostoru
hiperravn definisana kao $\theta_0 + \theta_1X_1 + \theta_2X_2 = 0$, a p-dimenzionalna hiperravan:
\begin{equation} \label{eq:hiperravan_eq}
  \theta_0 + \theta_1X_1 + \theta_2X_2 + \cdot\cdot\cdot + \theta_pX_p = 0
\end{equation}
Hiperravan možemo posmatrati kao ravan koja deli p-dimenzionalni prostor na dva
dela. U tom slučaju posmatramo (\ref{eq:hiperravan_eq}) kao nejednačine (\ref{eq:hiperravan_g}, \ref{eq:hiperravan_leq}), na osnovu koje
se određuje pripadnost klasi.
\begin{equation} \label{eq:hiperravan_g}
  \theta_0 + \theta_1X_1 + \theta_2X_2 + \cdot\cdot\cdot + \theta_pX_p > 0
\end{equation}
\begin{equation} \label{eq:hiperravan_leq}
  \theta_0 + \theta_1X_1 + \theta_2X_2 + \cdot\cdot\cdot + \theta_pX_p \leq 0
\end{equation}
Određivanje klase se jednostavno svodi na ispitivanje znaka leve strane
jednačine (\ref{eq:hiperravan_eq}).

\subsubsection{Klasifikovanje korišćenjem hiperravni}
Kako bi izvršili klasifikaciju, najpre nam je potrebna matrica podataka $X$, pri
čemu se ona sastoji od $n$ trening podataka u p-dimenzionalnom prostoru,
\begin{align}
    x_1 = \begin{pmatrix} x_{11} \\ x_{22} \\ \vdots \\ x_{1p} \end{pmatrix}, \ \cdot\cdot\cdot \ , \ x_n = \begin{pmatrix} x_{n1} \\ x_{n2} \\ \vdots \\ x_{np} \end{pmatrix}
\end{align}
i ova posmatranja pripadaju jednoj od dve klase - $y_1, \ y_2, \ y_n \in \{-1, \ 1\}$,
gde $-1$ predstavlja jednu, a $1$ drugu klasu. Pored toga potreban nam je i
skup podataka za testiranje $x^* \ = \ \left(x^{*}_{1}, \ x^{*}_{2}, \ \cdot\cdot\cdot \ , x^{*}_{n}\right)^T$. Naš cilj je razviti klasifikator
na osnovu trening podataka tako da tačno klasifikuje test podatke.

Sada (\ref{eq:hiperravan_g}) i (\ref{eq:hiperravan_leq}) možemo zapisati kao:
\begin{equation} \label{eq:hiperravan_g_if}
  \theta_0 + \theta_1X_{i1} + \theta_2X_{i2} + \cdot\cdot\cdot + \theta_pX_{ip} > 0 \ \ \ if \ \ y_i = 1,
\end{equation}
i
\begin{equation} \label{eq:hiperravan_leq_if}
  \theta_0 + \theta_1X_{i1} + \theta_2X_{i2} + \cdot\cdot\cdot + \theta_pX_{ip} \leq 0 \ \ \ if \ \ y_i = -1,
\end{equation}
Ekvivalentno, hiperravan ima osobinu
\begin{equation} \label{eq:hiperravan_property}
  y_i\left(\theta_0 + \theta_1X_{i1} + \theta_2X_{i2} + \cdot\cdot\cdot + \theta_pX_{ip}\right) > 0, \ \ \ \forall i = 1, \ \cdot\cdot\cdot \ , \ n.
\end{equation}

Sada možemo vršiti klasifikovanje test podataka na osnovu znaka funkcije
$$f\left(x^*\right) = \theta_0 + \theta_1x^{*}_{1} + \theta_2x^{*}_{2} + \ \cdot\cdot\cdot \ + \theta_px^{*}_{p}.$$
Ako je $f\left(x^*\right)$ pozitivno onda dodeljujemo podatku klasu 1, a ako ne onda -1.
Pored ove osobine, na osnovu apsolutne vrednosti $f\left(x^*\right)$ možemo odrediti i koliko je
trenutno posmatranje $x^*$ daleko od hiperravni. Mala vrednost $f\left(x^*\right)$ ukazuje da je velika
verovatnoća pogrešne klasifikacije, dok velika vrednost zanči da je posmatranje klasifikovano
sa velikom pouzdanošću.

\subsubsection{Klasifikator najveće margine - Maximal Margin Classifier}
Iz prethodnog izlaganja se može uočiti prvi problem idealnog postavljanja hiperravni.
U većini slučajeva, podaci mogu biti podeljeni na bezbroj načina, međutim, način odabira pozicije
hiperravni koji se je pokazao kao najefektivniji se svodi na slučaj kada je margina najveća.

Izračunavanje normalne distance svakog trening podataka nam može omogućiti idealno određivanje pozicije ravni, pri čemu se
najmanja takva distanca naziva marginom. Hiperravan na osnovu koje se vrši podela je određena tako da je margina najveća,
tj. hiperravan za koju je najudaljenija minimalna distanca nad trening podacima. Nakon toga, možemo klasifikovati podatke
na osnovu koje strane se nalaze test podaci u odnosu na hiperravan. Ovakav način klasifikacije predstavlja \textit{maximal margin classifier}.

\textit{Maximal margin classifier} klasifikuje test podatke $x^*$ na osnovu znaka funkcije $f\left(x^*\right) = \theta_0 + \theta_1x^{*}_{1} + \theta_2x^{*}_{2} + \ \cdot\cdot\cdot \ + \theta_px^{*}_{p}$,
gde su $\theta_0, \ \theta_1, \ \cdot\cdot\cdot \ , \ \theta_p$ koeficijenti \textit{maximal margin} hiperravni.

U ovom slučaju, trening podaci koji razdvajaju određuju širinu margine se nazivaju potporni vektori - \textit{support vectors}.
Može se uočiti da samo potporni vektori određuju položaj hiperravni što nam daje brojne pogodnosti.

\subsubsection{Konstrukcija klasifikatora}
Proces konstruisanja klasifikatora za $n$ trening podataka $x_1, x_2,\cdot\cdot\cdot,x_n \in R^p$
sa dodeljenim labelama $y_1, y_2, \cdot\cdot\cdot, y_n \in \{-1,1\}$ koristi
optimizaciju kako bi se odredio položaj hiperravni. Optimizacija se svodi na
\begin{equation} \label{eq:svm_max_m} \max\limits_{\theta_0, \cdot\cdot\cdot, \theta_p} \ M \end{equation}
\begin{equation} \label{eq:svm_sum} \sum_{j=1}^{p}\theta_{j}^2=1 \end{equation}
\begin{equation} \label{eq:svm_constraint} y_i\left(\theta_0 + \theta_1X_{i1} + \theta_2X_{i2} + \cdot\cdot\cdot + \theta_pX_{ip}\right) \geq M \ \ \ \forall i = 1, \cdot\cdot\cdot, n \end{equation}
Pri čemu ograničenje (\ref{eq:svm_constraint}) garantuje da će svaki podatak biti na ispravnoj strani hiperravni, (\ref{eq:svm_sum}) dodaje ograničenje da je normalna distanca od i-tog podatka
do hiperravni definisana sa $$y_i\left(\theta_0 + \theta_1X_{i1} + \theta_2X_{i2} + \cdot\cdot\cdot + \theta_pX_{ip}\right).$$ Tako da ograničenja (\ref{eq:svm_constraint}) i (\ref{eq:svm_sum}) omogućavanju
da se podatak nađe sa korektne strane i na najmanjoj distanci $M$ od hiperravni. $M$ predstavlja marginu hiperravni koju je potrebno maksimizovati
podešavanjuće parametre $\theta_0, \ \theta_1, \ \cdot\cdot\cdot \ , \ \theta_p$. Transformisanjem navedenih funkcija se može dobiti oblik koji je pogodniji za optimizaciju, problem koji se u literaturi naziva
\textit{kvadratnim optimizacionim problemom sa linearnim ograničenjima}.

Još jedan problem određivanja hiperravni se javlja u slučajevima kada nije moguće podeliti prostor.
Za takav skup podataka kžemo da nije linearno separabilan, i u ovom slučaju optimizacioni problem (\ref{eq:svm_max_m},\ \ref{eq:svm_sum},\ \ref{eq:svm_constraint}) nema rešenja.

U nastavku će biti obrađen princip meke margine koji proširuje koncept podele prostora uz pomoć hiperravni.

Generalizacija klasifikatora sa maksimalnom marginom na neseparabilnom skupu podataka se naziva klasifikator potpornih vektora (\textit{support vector classifier}).

\subsection{Klasifikator potpornih vektora - support vector classifier}
Do sada smo razmatrali slučajeve u kojima se podaci mogu linearno razdvojiti, međutim ovo nije čest slučaj.
Da bi postigli veću robusnost i veću preciznost klasifikovanja dozvolićemo da nekoliko podatka bude pogrešno klasifikovano.
Zbog ove osobine se klasifikator baziran na potpornim vektorima često naziva \textit{soft margin classifier}.

U ovom slučaju optimizacioni problem se svodi na:
\begin{equation} \label{eq:svm_op_max_m} \max\limits_{\theta_0, \cdot\cdot\cdot, \theta_p, \epsilon_1, \cdot\cdot\cdot, \epsilon_n} \ M \end{equation}
\begin{equation} \label{eq:svm_op_sum} \sum_{j=1}^{p}\theta_{j}^2=1 \end{equation}
\begin{equation} \label{eq:svm_op_constraint} y_i\left(\theta_0 + \theta_1X_{i1} + \theta_2X_{i2} + \cdot\cdot\cdot + \theta_pX_{ip}\right) \geq M\left(1-\epsilon_i\right) \ \ \ \forall i = 1, \cdot\cdot\cdot, n \end{equation}
\begin{equation} \label{eq:svm_op_epsilon} \sum_{j=1}^{n}\epsilon_{j}^2 \leq C, \ \epsilon_i \geq 0 \end{equation}
gde je $C$ pozitivna konstanta za podešavanje margine, $M$ je širina margine, a $\epsilon_1, \cdot\cdot\cdot, \epsilon_n$
su promenljive koje nam govore gde se nalazi i-ti podatak (\ref{eq:svm_iti_podatak}). Isto kao i u prethodnom slučaju, test podatak $x^*$ će biti klasifikovan na osnovu znaka funkcije
$f\left(x^*\right) = \theta_0 + \theta_1x^{*}_{1} + \theta_2x^{*}_{2} + \ \cdot\cdot\cdot \ + \theta_px^{*}_{p}.$

\begin{equation} \label{eq:svm_iti_podatak}
  i-ti \ podatak \ se \ nalazi \ na
    \begin{cases}
      korektnoj \ strani & \text{if}\ \epsilon_i = 0 \\
      nekorektnoj \ strani \ margine & \text{if}\ \epsilon_i > 0 \\
      nekorektnoj \ strani \ hiperravni & \text{if}\ \epsilon_i > 1 \\
    \end{cases}
\end{equation}
Sa druge strane, parametar $C$ ograničava broj pogrešno klasifikovanih podatak koje možemo tolerisati. Drugim rečima, $C$ kontroliše širinu margine,
margina je šira kada je $C$ veće. U praksi se najčešće ovaj parametar bira u fazi ukrštene validacije, i predstavlja parametar za kontrolu \textit{bias-variance tradeoff}-a.
Ako je $C$ veliko, onda veliki broj podataka učestvuje u određivanju hiperravni, što znači da imamo malu varijansu i veliki bajas, dok malo $C$ nam govori da imamo mali bajas, ali zato veliku varijansu.

Klasifikator potpornih vektora ima veliku robusnost zahvaljujući odlučivanju o granici odlučivanja samo na osnovu malog broja potpornih vektora.
Ova osobina predstavlja glavnu razliku u odnosu na LDA čija granica odlučivanja zavisi od srednje vrednosti svakog podatka, sa druge strane logistička regresija je dosta
slična klasifikatoru baziranom na potpornim vektorima.
