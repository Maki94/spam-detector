\section{Linearna diskriminentna analiza - LDA}
Osnovni nedostatak prethodnog modela je nepogodnost predikcije nebinarnog izlaza.
Ovo nije slučaj sa Linearnom diskriminentnom analizom. Pored ove razlike,
logistička regresija je vrlo nestabilna sa dobro odvojenim klasama. LDA je
stabilniji klasifajer i kada je broj prediktora $X$ mali i ima približno normalnu
distribuciju.

Za razliku od logističke regresije koja direktno modelira verovatnoću $Pr\left(Y=k | X = x\right)$
koristeći logističku funkciju (\ref{eq:logistic_function}), LDA ima manje
direktan pristup. Naime, procena
verovatnoće se vrši uz modeliranje distribucije prediktora X odvojeno za svaku
od rezltujućih klasa $Y$, i nakon toga, koristi Bajesove teoreme za
prebacivanje ovih procena u rezultujuću verovatnoću $Pr\left(Y=k | X = x\right)$.
Kada su distribucije $X$ normalne onda je model sličan logističkoj regresiji.

\subsection{Bajesova teorema za klasifikaciju}
Pretpostavimo da želimo da klasifikujemo podatke u $k$ klasa, $k \geq 2$. Sada
kvantitivni izlaz $Y$ može imati $k$ različitih vrednosti. Uzmimo da $\pi_k$
predstavlja verovatnoću da posmatranje $x_i$ pripada klasi k, i da
$f_k\left(X\right) \equiv Pr\left(X=x | Y=k\right)$ označava funkciju gustine
od $X$ da jedno posmatranje pripada klasi $k$. Prema Bajesovoj teorimi:
\begin{equation} \label{eq:lda_bayes}
  p_k\left(x\right) = Pr(X=x | Y=k) =
  \frac
    {\pi_k f_k\left(x\right)}
    {\sum_{l=1}^{K}\pi_l f_l\left(x\right)}
\end{equation}
$\pi_k$ predstavlja frakciju trening podataka koji pripadaju trening skupu, dok
je procena $f_k\left(X\right)$ malo zahtevnija. Procenom ove funkcije dobijamo
verovatnoću za klasifikovanje podataka.

\subsection{LDA za jedan prediktor $p = 1$}
Pretpostavimo da imamo samo jedan prediktor. Želimo da procenimo
$f_k\left(x\right)$ kako bismo uz pomoć (\ref{eq:lda_bayes}) odredili
$p_k\left(x\right)$ i klasifikovali podatak $x$ određenoj klasi za koju je
$p_k\left(x\right)$ najveći. Da bi procenili $f_k\left(x\right)$,
napravićemo par pretpostavki.

Smatrajmo da $f_k\left(x\right)$ ima Gausovu raspodelu. U jednodimenzionalnim
uslovima normalna gustina ima oblik:
\begin{equation} \label{eq:lda_gaussian}
  f_k\left(x\right) =
    \frac {1} {\sqrt{2\pi}\sigma_k}
    e^{-\frac{1}{2 \sigma_k^2} (x - \mu_k)^2}
\end{equation}
gde je $\mu_k$ srednja vrednost, a $\sigma_k^2$ varijansa za $k$-tu klasu. Zatim
smatrajmo da imamo istu varijansu za svaku klasu
($\sigma^2 = \sigma_1^2 = \sigma_2^2 = \cdot\cdot\cdot = \sigma_k^2$). Iz
(\ref{eq:lda_bayes}) i (\ref{eq:lda_gaussian}) dobijamo:
\begin{equation} \label{eq:lda_p_k_normal}
  p_k\left(x\right) =
    \frac
    {\pi_k \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2 \sigma^2} (x - \mu_k)^2}}
    {\sum_{l=1}^{K} \pi_l \frac{1}{\sqrt{2\pi}\sigma} e^{-\frac{1}{2 \sigma^2} (x - \mu_l)^2}}
\end{equation}
Bajesov klasifajer dodeljuje jednom podatku $X = x$ klasu za koji je
(\ref{eq:lda_p_k_normal}) najveći. Logaritmovanjem (\ref{eq:lda_p_k_normal}) i
sređivanjem izraza dobijamo ekvivalentan zapis
\begin{equation} \label{eq:lda_delta_k_normal}
  \delta_k\left(x\right) =
    x\frac{\mu_k}{\sigma^2} - \frac{\mu_k^2}{2\sigma^2} + \log{\pi_k}
\end{equation}
za koji je $\delta_k\left(x\right)$ najveći. \\

Sada je potrebno proceniti $\pi_k$, $\mu_k$ i $\sigma^2$ kako bi izračunali $\delta_k\left(x\right)$:
\begin{equation} \label{eq:lda_mu} \hat{\mu} = \frac{1}{n_k} \sum_{i:y_i=k} x_i \end{equation}
\begin{equation} \label{eq:lda_sigma} \hat{\sigma}^2 = \frac{1}{n - K} \sum_{k=1}^{K} \sum_{i:y_i=k} (x_i-\hat{\mu_k})^2 \end{equation}
\begin{equation} \label{eq:lda_pi} \hat{\pi_k} = \frac{n_k}{n} \end{equation}
$n$ je ukupan broj trening podataka, $n_k$ broj trening podataka koji
pripadaju klasi $k$, $\mu_k$ srednja vrednost svih trening podataka iz klase $k$,
a $\sigma$ težinska srednja vrednost varijanse za svaku od K klasa. \\

Integrisanjem (\ref{eq:lda_mu}), (\ref{eq:lda_sigma}) i (\ref{eq:lda_pi}) u
(\ref{eq:lda_delta_k_normal}) dobijamo procenenju vrednost $\hat{\delta}_k\left(x\right)$
na osnovu čije maksimalne vrednosti dodeljujemo klasu ulaznom podataku $X = x$.
\begin{equation} \label{eq:lda_delta_k_lda}
  \hat{\delta}_k\left(x\right) =
    x\frac{\hat{\mu_k}}{\hat{\sigma}^2} - \frac{\hat{\mu_k}^2}{2\hat{\sigma}^2} + \log{\hat{\pi_k}}
\end{equation}
Naziv linearan u LDA upravo potiče iz činjenice da je diskrimitivna
funkcija $\hat{\delta_k}\left(x\right)$ u (\ref{eq:lda_delta_k_lda}) linearna po
$x$.

\subsection{LDA za veći broj prediktora $p > 1$}

Za veći broj prediktora $X = (X_1, X_2, \cdot\cdot\cdot, X_p)$ primenićemo
multivariacionu normalnu distribiciju sa specifičnim vektorom srednjih vrednosti
i zajedničkom matricom kovarjansi.

P-dimenzionalni vektor $X$ ima multivariacionu normalnu distribiciju i obeležavamo
sa $X \sim N(\mu, \sum)$, srednja vrednost od X je $E(X)=\mu$ i $p$ x $p$ matrica
kovarjansi $Cov(X)=\sum$. Za ove parametre dobijamo multivariacionu Gausovu funkciju gustine:
\begin{equation} \label{eq:lda_multi_var_gauss_density}
  f(x) =
  \frac{1}{(2\pi)^{\pi/2}|\sum|^{1/2}}
  e^{-\frac{1}{2}(x-\mu)^T\sum^{-1}(x-\mu)}
\end{equation}
sada, integrisanjem ove funkcije gustine u (\ref{eq:lda_bayes}) dobijamo funkciju
čijem ulazu $x$ vrednost je klasu za koju je vrednost:
\begin{equation} \label{}
  \delta_k\left(x\right) =
    x^T\sum^{-1}\mu_k -\frac{1}{2}\mu_k^T\sum^{-1}\mu_k + \log{\pi_k}
\end{equation}
najveća.

\subsection{Kvadratna diskriminentna anliza - QDA}

Nedostatak LDA-a je koršćenje iste matrice kovarjansi za svaku klasu. Kod QDA-a
svaka klasa ima svoju mtricu kovarjansi.  Podatak iz klase $k$ je oblika
$X \sim N\left(\mu_k, \sum_k\right)$, gde je $\sum_k$ matrica kovarjanse za
$k$-tu klasu. Bajesov klasifajer dodeljuje podatak klasi za koju je
\begin{equation}
  \hat{\delta_k}\left(x\right) =
  -\frac{1}{2}(x-\mu_k)^T\sum_{k}^{-1}(x-\mu_k)+\log{\pi_k}
\end{equation}
najveće. Ovaj funkcija je kvadratna, otuda i naziv quadratic discriminant
analysis. \\

Zašto QDA klasifikuje podatke sa većom pouzdanošću u odnosu na LDA, odgovor leži
u bias-variance trade-off. Ako je potrebno proceniti p prediktora, onda procena
matrice kovarjansi zahteva procenu $p(p+1)/2$ parametara, QDA procenjuje za svaku
matricu posebno, $K*p(p+1)/2$. Za veliki broj parametara ovo može biti vremenski
veoma zahtevno. \\

Trade-off: LDA smatra da je matrica kovarjansi ista između svih klasa, što je
loše zbog toga što LDA može patiti od visokog bias-a. LDA je bolji od QDA kada je
mali broj trening podataka i smanjivanje varijanse je krucijalno. Sa druge strane
QDA je preporučljiv kada je obiman trening skup, tako da varijansa klasifajera
nije u prvom planu.
Sa druge strane, LDA je manje fleksibilan što se ugleda u manjoj
varijansi, što zanči da se kod LDA može javiti problem visokog bias-a.
